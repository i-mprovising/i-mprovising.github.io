---
layout: post
title: "Sequence to Sequence Learning with Neural Networks"
date: 2023-08-22 15:13:18 +0900
tags: [NLP, seq2seq]
categories: papers
---
## Introduction
본 논문의 도입부에서는 DNN의 장점과 한계를 말해주고 있습니다. 다양한 task에서 좋은 성과를 보이는 DNN이지만, input과 output vector의 크기가 고정된 경우에 대해서만 작동이 가능했습니다. 하지만 QA(Question Answering), MT(Machine Translation)와 같은 대부분의 real-word task에서는 input과 output의 크기를 미리 알기 어렵습니다. 따라서 이 논문에서는 이를 해결하기 위해 general한 sequence to sequence를 mapping하는 모델을 제시합니다. 

모델은 2개의 LSTM으로 이루어진 구조를 갖습니다. 첫 번째 LSTM은 주어진 input 벡터를 고정된 크기의 벡터로 바꿔줍니다. 두 번째 LSTM은 첫 번째 LSTM의 마지막 hidden layer에서 나온 값을 input으로 받은 뒤  output을 뽑아내는 역할을 합니다. seq2seq 구조에서 input으로 넣어주는 부분을 source sentence, output으로 나오는 값을 target sentence라고 합니다. source sentence의 순서를 뒤집어서 모델에게 주는 방식이 seq2seq에서 문제시되는 long-term dependency를 잘 해결한다고 합니다.
## The Model
기존 RNN은 input, output sequence의 길이가 정해진 상황에서 seq2seq로서의 활용이 가능했습니다. 이를 해결하기 위해 [Cho et al.](https://arxiv.org/abs/1406.1078)에서는 먼저 input sequence를 RNN을 통해 고정된 크기의 벡터로 매핑합니다. 그 후 두 번째 RNN으로 target sequence를 얻어냅니다. 하지만 이 방법은 long-term dependency 문제로 인해 RNN의 효과적인 학습이 어렵다고 본 논문에서 말하고 있습니다. 따라서 이 논문에서 제시하는 방법으로는 RNN 대신 LSTM을 사용하여 long-term dependency 문제를 해결합니다.  
![]({{site.baseurl}}/images/seq2seq.jpeg)
이 구조는 첫 번째 LSTM에서 input sequence를 받아 고정된 크기의 벡터 v로 바꿔주고, 마지막 hidden layer에서 이 v를 다음 LSTM으로 넘겨줍니다. 두 번째 LSTM은 v를 받아 조건부 확률을 추정합니다. 
## Experiments
### Decoding
Beam search를 이용하여 디코딩을 진행합니다. B(=beam size)가 2일 때 가장 결과가 좋았지만, B = 1일 때도 괜찮은 결과를 보였습니다. 
### Reversing the Source Sentence
원리를 정확하게 설명하지는 못하지만, source sentence의 순서를 뒤집는 것이 모델의 성능을 높였다고 합니다. 순서를 뒤집기 전과 후를 비교했을 때, perplexity 점수가 5.8에서 4.7로 개선되었고, BLEU score는 25.9에서 30.6으로 향상되었습니다. 

*💡 Perplexity(PPL) : 언어 모델의 평가 지표. 점수가 낮을수록 좋은 평가입니다.*

Source sentence의 순서를 뒤집는 것은 short-term depedency를 가져온다고 합니다. Source sentence와 target setence에서 서로 대응되는 각 단어 사이의 평균 거리는 뒤집기 전이나 후나 동일합니다. 그러나 뒤집은 후는 “minimal time lag”를 크게 줄여줍니다. 이는 “establising communication”을 형성하여 학습에 도움이 되고, 메모리 활용을 개선한다고 합니다.
